# Contemporary Data Processing Systems - Map-Reduce Engine

A distributed task processing system implementing the **map-reduce programming model** for parallel data processing across multiple nodes.

## ğŸ¯ Project Overview

This project implements a complete distributed computing framework that:

- âœ… Distributes processing across **4+ physical nodes** (configurable)
- âœ… Supports **map**, **shuffle**, and **reduce** transformations
- âœ… Implements **direct worker-to-worker communication** during shuffle (no central bottleneck)
- âœ… Provides a flexible API for implementing custom map-reduce problems
- âœ… Includes **two complex examples** (beyond WordCount):
  - **Web Server Log Analysis**: Analyzes access logs to compute endpoint statistics, response times, error rates, and traffic patterns
  - **E-commerce Sales Analysis**: Processes transactions to generate revenue reports, product rankings, and customer insights
- âœ… Offers both **CLI** and **programmatic API** interfaces
- âœ… Features comprehensive monitoring and progress tracking

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Coordinator (Master)                    â”‚
â”‚  - Job orchestration                                        â”‚
â”‚  - Worker registry & health monitoring                      â”‚
â”‚  - Data distribution                                        â”‚
â”‚  - Result collection                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Distributes tasks & monitors
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Worker 1â”‚      â”‚Worker 2â”‚     â”‚Worker 3â”‚    â”‚Worker 4â”‚
â”‚        â”‚â—„â”€â”€â”€â”€â–ºâ”‚        â”‚â—„â”€â”€â”€â–ºâ”‚        â”‚â—„â”€â”€â–ºâ”‚        â”‚
â”‚ Map    â”‚      â”‚ Map    â”‚     â”‚ Map    â”‚    â”‚ Map    â”‚
â”‚Shuffle â”‚      â”‚Shuffle â”‚     â”‚Shuffle â”‚    â”‚Shuffle â”‚
â”‚Reduce  â”‚      â”‚Reduce  â”‚     â”‚Reduce  â”‚    â”‚Reduce  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²              â–²              â–²             â–²
     â”‚              â”‚              â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           Direct worker-to-worker 
           communication (shuffle)
```

### Map-Reduce Workflow

1. **Input Distribution**: Coordinator splits input data across workers
2. **Map Phase**: Each worker applies the mapper to its data partition
   - Emits intermediate key-value pairs
3. **Shuffle Phase**: Workers communicate directly to redistribute data
   - Each key is sent to a specific worker (determined by hash partitioning)
   - **No data passes through coordinator** (efficient!)
4. **Reduce Phase**: Each worker aggregates values for its assigned keys
5. **Result Collection**: Coordinator gathers final results from all workers

## ğŸ“‹ Prerequisites

- Python 3.8+
- Network connectivity between nodes
- Sufficient disk space for intermediate data

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd Contemporary-Data-Processing-Systems-Project

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

Edit `config.yaml` to define your cluster topology:

```yaml
cluster:
  coordinator:
    host: "localhost"
    port: 5000
    
  workers:
    - id: "worker-1"
      host: "localhost"
      port: 5001
      data_dir: "./data/worker1"
    
    - id: "worker-2"
      host: "localhost"
      port: 5002
      data_dir: "./data/worker2"
    # ... add more workers
```

**For distributed deployment**: Replace `localhost` with actual IP addresses of your cluster nodes.

### 3. Running the System

#### Option A: Quick Demo (Single Machine)

Run a complete example with automatic cluster startup:

```bash
# Log analysis (10,000 records)
python run_example.py log-analysis 10000

# Sales analysis (5,000 records)
python run_example.py sales-analysis 5000
```

#### Option B: Manual Cluster Management

**Terminal 1 - Start Worker 1:**
```bash
python main.py start-worker worker-1
```

**Terminal 2 - Start Worker 2:**
```bash
python main.py start-worker worker-2
```

**Terminal 3 - Start Worker 3:**
```bash
python main.py start-worker worker-3
```

**Terminal 4 - Start Worker 4:**
```bash
python main.py start-worker worker-4
```

**Terminal 5 - Check Cluster Status:**
```bash
python main.py status
```

#### Option C: Distributed Deployment (4+ Physical Machines)

**On each worker machine:**
```bash
# Machine 1
python main.py start-worker worker-1

# Machine 2
python main.py start-worker worker-2

# Machine 3
python main.py start-worker worker-3

# Machine 4
python main.py start-worker worker-4
```

**On coordinator machine:**
```bash
python run_example.py log-analysis 50000
```

## ğŸ“Š Example Problems

### 1. Web Server Log Analysis

**Problem**: Analyze millions of web server access logs to compute:
- Request counts per endpoint
- Average/min/max response times
- Error rates (4xx/5xx responses)
- Peak traffic hours
- HTTP method distribution

**Input Format**:
```
2024-11-17 10:23:45, 192.168.1.100, GET, /api/users, 200, 145
2024-11-17 10:24:12, 192.168.1.101, POST, /api/orders, 201, 289
```

**Map Phase**: Parse logs â†’ Emit `(endpoint, stats)`

**Shuffle Phase**: Group all statistics by endpoint across workers

**Reduce Phase**: Aggregate statistics per endpoint

**Sample Output**:
```
Endpoint                       Requests   Avg Time     Error Rate   Peak Hours
------------------------------------------------------------------------------
/api/products                  2,431      187.32 ms    3.21%        10, 14, 16
/api/users                     1,892      145.67 ms    2.15%        11, 13, 15
/api/orders                    1,543      289.45 ms    5.67%        12, 14, 17
```

### 2. E-commerce Sales Analysis

**Problem**: Analyze transaction data to compute:
- Revenue by product category
- Top-selling products
- Regional sales distribution
- Customer purchase patterns

**Input Format**:
```
TXN001, 2024-11-17 10:30:00, CUST123, PROD456, Electronics, 2, 299.99, North
```

**Map Phase**: Parse transaction â†’ Emit multiple keys:
- `(category:Electronics, revenue_data)`
- `(product:PROD456, sales_data)`
- `(region:North, revenue_data)`
- `(customer:CUST123, purchase_data)`

**Shuffle Phase**: Group all related records by type and identifier

**Reduce Phase**: Aggregate by category, product, region, and customer

**Sample Output**:
```
Top Categories by Revenue:
  Electronics          $1,234,567.89  (5,234 transactions, 8,901 units)
  Clothing             $987,654.32    (8,123 transactions, 15,234 units)
  
Revenue by Region:
  North                $2,345,678.90  (6,789 transactions)
  South                $1,987,654.32  (5,432 transactions)
```

## ğŸ”§ Creating Custom Map-Reduce Problems

### Step 1: Define Your Mapper

```python
from src.core.base import Mapper
from typing import Any, Iterator, Tuple

class MyMapper(Mapper):
    def map(self, key: Any, value: Any) -> Iterator[Tuple[Any, Any]]:
        """
        Process input and emit intermediate key-value pairs.
        
        Args:
            key: Input key (e.g., line number)
            value: Input value (e.g., line content)
            
        Yields:
            (intermediate_key, intermediate_value) tuples
        """
        # Your mapping logic here
        words = value.split()
        for word in words:
            yield (word.lower(), 1)
```

### Step 2: Define Your Reducer

```python
from src.core.base import Reducer
from typing import Any, Iterator, List, Tuple

class MyReducer(Reducer):
    def reduce(self, key: Any, values: List[Any]) -> Iterator[Tuple[Any, Any]]:
        """
        Aggregate values for each key.
        
        Args:
            key: The key to reduce
            values: All values associated with this key
            
        Yields:
            (output_key, output_value) tuples
        """
        # Your reduction logic here
        total = sum(values)
        yield (key, total)
```

### Step 3: Run Your Job

```python
from src.core.coordinator import Coordinator
from src.core.worker import Worker
import threading

# Start workers (in production, these would be on separate machines)
workers = []
for worker_config in config['cluster']['workers']:
    worker = Worker(...)
    worker.set_mapper(MyMapper())
    worker.set_reducer(MyReducer())
    workers.append(worker)
    
    thread = threading.Thread(target=worker.start, daemon=True)
    thread.start()

# Create coordinator and execute job
coordinator = Coordinator(...)
results = coordinator.execute_job(
    input_data=my_data,
    mapper=MyMapper(),
    reducer=MyReducer()
)

# Process results
for key, value in results:
    print(f"{key}: {value}")
```

## ğŸ“ Project Structure

```
Contemporary-Data-Processing-Systems-Project/
â”œâ”€â”€ config.yaml                 # Cluster configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ main.py                     # CLI interface
â”œâ”€â”€ run_example.py              # Example runner script
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                   # Core map-reduce engine
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract base classes
â”‚   â”‚   â”œâ”€â”€ worker.py          # Worker node implementation
â”‚   â”‚   â”œâ”€â”€ coordinator.py     # Coordinator implementation
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ examples/              # Example problems
â”‚   â”‚   â”œâ”€â”€ log_analysis.py   # Web server log analysis
â”‚   â”‚   â”œâ”€â”€ sales_analysis.py # E-commerce sales analysis
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â””â”€â”€ data/                      # Worker data directories (created at runtime)
    â”œâ”€â”€ worker1/
    â”œâ”€â”€ worker2/
    â”œâ”€â”€ worker3/
    â””â”€â”€ worker4/
```

## ğŸ” Key Features

### Direct Worker-to-Worker Shuffle

Unlike traditional map-reduce implementations where intermediate data flows through a central coordinator, our implementation uses **direct peer-to-peer communication**:

```python
# In Worker.shuffle_data()
for target_worker_id, data in partitions.items():
    if target_worker_id == self.worker_id:
        # Keep local data
        self.reduce_input[key].append(value)
    else:
        # Send directly to remote worker (no coordinator!)
        worker_info = self.workers[target_worker_id]
        url = f"http://{worker_info['host']}:{worker_info['port']}/shuffle"
        requests.post(url, json={'data': data})
```

**Benefits**:
- âœ… Eliminates coordinator bottleneck
- âœ… Better scalability
- âœ… Reduced network latency
- âœ… Load balancing across network

### Hash-Based Partitioning

Ensures even distribution of keys across workers:

```python
class HashPartitioner(Partitioner):
    def get_partition(self, key: Any, num_partitions: int) -> int:
        return hash(key) % num_partitions
```

### Comprehensive Monitoring

- Health checks for all workers
- Progress tracking for each phase (map, shuffle, reduce)
- Timing statistics
- Record counts and throughput metrics

## ğŸ§ª Testing

```bash
# Run with small dataset for testing
python run_example.py log-analysis 100

# Run with larger dataset for performance testing
python run_example.py log-analysis 100000

# Check cluster status
python main.py status
```

## ğŸ“ˆ Performance Considerations

- **Data Size**: The system can handle datasets from thousands to millions of records
- **Worker Count**: More workers = better parallelism (recommended: 4-8 workers)
- **Network**: Fast network between workers improves shuffle performance
- **Memory**: Each worker needs enough RAM to hold its partition + intermediate data

## ğŸ› Troubleshooting

### Workers not starting
- Check port availability: `netstat -an | grep <port>`
- Verify firewall settings
- Ensure `data/` directories are writable

### Connection errors during shuffle
- Verify network connectivity between worker nodes
- Check firewall rules allow HTTP traffic on worker ports
- Ensure correct IP addresses in `config.yaml`

### Out of memory errors
- Reduce dataset size
- Increase worker count to distribute load
- Adjust `shuffle_buffer_size` in config

## ğŸ“š Additional Documentation

### Coordinator API

```python
coordinator = Coordinator(host="localhost", port=5000)
coordinator.register_worker(worker_id, host, port, data_dir)
coordinator.check_workers_health()
coordinator.execute_job(input_data, mapper, reducer, partitioner)
```

### Worker API

```python
worker = Worker(worker_id, host, port, data_dir)
worker.set_mapper(mapper_instance)
worker.set_reducer(reducer_instance)
worker.set_partitioner(partitioner_instance)
worker.start()  # Blocks and runs HTTP server
```

## ğŸ“ Project Requirements Checklist

- âœ… Distributed on 4+ physical nodes (configurable)
- âœ… Processes distributed datasets
- âœ… Supports map, filter, reduce transformations
- âœ… Implements shuffle with direct worker communication
- âœ… Configurable via YAML and CLI
- âœ… Generates output files with results
- âœ… Includes complex examples beyond WordCount
- âœ… Console application with progress monitoring
- âœ… Comprehensive documentation

## ğŸ“ License

This project is created for educational purposes as part of the Contemporary Data Processing Systems course.

## ğŸ‘¥ Contributors

Your Name - Your Group

---

**Happy Distributed Computing! ğŸš€**